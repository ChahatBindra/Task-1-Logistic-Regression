# -*- coding: utf-8 -*-
"""Vasu_Gupta_E18CSE199_AI_ML_task_2_logistic_diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6esrJwYVjBVmgaKu5DAVOfcUkEHMu2w
"""



"""Part 2.1: Getting familiar with linear algebraic functionsÂ¶"""

import numpy as np
arr = np.random.randint(1,20,size = (10,10))

arr

"""Inverse"""

inv = np.linalg.inv(arr)
print(inv)

"""Dot product"""

arr_tran = np.transpose(arr)
print(arr_tran)

Mult = np.dot(arr,arr_tran)
print(Mult)

"""Decompose the original matrix using eigen decomposition print the eigen values and eigen vectors"""

import numpy as np
w, v = np.linalg.eig(arr)
print("EigenValues")
print(w)
print("...............................")

print("Eigenvectors")
print(v)

"""Jacobbian Matrix"""

import autograd.numpy as np
def cost(x):
  y = x**3
  return y

import autograd
jac = autograd.jacobian(cost)
hess = autograd.hessian(cost)

print("................Jacobian.............")
print(jac(np.array(arr)))
print("")
print("................Hessian..............")
print(hess(np.array(arr)))

"""## Part 1 from Scratch

lloading and printing data
"""

url = "https://raw.githubusercontent.com/DeepConnectAI/challenge-week-3/master/data/diabetes_data.csv"
import pandas as pd
data = pd.read_csv(url)

data

print(data[0:5])

print(data[-5:])



"""Label Encoding"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cols = list(data.columns)
data[cols[1:]] = data[cols[1:]].apply(lambda col: le.fit_transform(col))

data

"""Scaling of Age column"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(data)
data = scaler.transform(data)

x = data[:,:-1]
y = data[:,-1]

"""Splitting of the dataset"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.4, random_state=0)

import numpy as np

def predict(X, weights):
    '''Predict class for X.
    For the given dataset, predicted vector has only values 0/1
    Args:
        X : Numpy array (num_samples, num_features)
        weights : Model weights for logistic regression
    Returns:
        Binary predictions : (num_samples,)
    '''

    ### START CODE HERE ###
    z  = np.dot(X,weights)
    logits = sigmoid(z)
    y_pred = np.array(list(map(lambda x: 1 if x>0.5 else 0, logits)))
    ### END CODE HERE ###
    
    return y_pred

def sigmoid(z):
        '''Sigmoid function: f:R->(0,1)
        Args:
            z : A numpy array (num_samples,)
        Returns:
            A numpy array where sigmoid function applied to every element
        '''
        ### START CODE HERE
        sig_z = (1/(1+np.exp(-z)))
        ### END CODE HERE
        
        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'
        return sig_z

def cross_entropy_loss(y_true, y_pred):
    '''Calculate cross entropy loss
    Note: Cross entropy is defined for multiple classes/labels as well
    but for this dataset we only need binary cross entropy loss
    Args:
        y_true : Numpy array of true values (0/1) of size (num_samples,)
        y_pred : Numpy array of predicted values (probabilites) of size (num_samples,)
    Returns:
        Cross entropy loss: A scalar value
    '''
    # Fix 0 values in y_pred
    y_pred = np.maximum(np.full(y_pred.shape, 1e-7), np.minimum(np.full(y_pred.shape, 1-1e-7), y_pred))
    
    ### START CODE HERE
    ce_loss = np.mean((-y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred)))
    ### END CODE HERE
    
    return ce_loss

def newton_optimization(X, y, max_iterations=100):
    '''Implement netwon method for optimizing weights
    Args:
        X : Numpy array (num_samples, num_features)
        max_iterations : Max iterations to update the weights
    Returns:
        Optimal weights (num_features,)
    '''
    num_samples = X.shape[0]
    num_features = X.shape[1]
    # Initialize random weights
    weights = np.zeros(num_features,)
    # Initialize losses
    losses = []
    
    # Newton Method
    for i in range(max_iterations):
        # Predict/Calculate probabilties using sigmoid function
        z = np.dot(X, weights)
        y_p = sigmoid(z)
        
        # Define gradient for J (cost function) i.e. cross entropy loss
        gradient = 1./num_samples * np.dot(X.T,(y_p - y))
        
        # Define hessian matrix for cross entropy loss
        hessian = 1./num_samples * X.T.dot(np.diag(y_p*(1 - y_p))).dot(X)
        
        # Update the model using hessian matrix and gradient computed
        weights -= np.dot(np.linalg.pinv(hessian),gradient)
        
        # Calculate cross entropy loss
        loss = cross_entropy_loss(y, y_p)
        # Append it
        losses.append(loss)

    return weights, losses

"""#Train Weights"""



# Train weights
weights, losses = newton_optimization(x_train, y_train)

"""#Plottiing Loss curve"""

import matplotlib.pyplot as plt

# Plot the loss curve
plt.plot([i+1 for i in range(len(losses))], losses)
plt.title("Loss curve")
plt.xlabel("Iteration num")
plt.ylabel("Cross entropy curve")
plt.show()

from sklearn.metrics import accuracy_score
our_model_test_acuracy = accuracy_score(y_test, predict(x_test, weights))

print(f"\nAccuracy in testing set by our model: {our_model_test_acuracy}")



"""#Compare with the scikit learn implementation"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(x_train,y_train)

"""prediction on test set"""

y_pred = model.predict(x_test)

"""Accuracy on test data"""

sklearn_test_accuracy = accuracy_score(y_test, y_pred)

print(f"\nAccuracy in testing set by sklearn model: {sklearn_test_accuracy}")

